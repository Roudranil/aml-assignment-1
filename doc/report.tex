\documentclass[12pt]{scrartcl}
\usepackage[T1]{fontenc}
\usepackage[latin1]{inputenc}
\usepackage[margin=0.9in]{geometry}
\usepackage{esvect}
\usepackage{enumitem}
\usepackage{amsthm, amsmath, amssymb}
\usepackage{indentfirst}
\usepackage[affil-it]{authblk}
\usepackage{xfrac}
\usepackage{nicefrac}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{float}
\usepackage{soul}
\usepackage{xcolor}
\usepackage{lmodern}
\usepackage{realboxes}
\usepackage{tcolorbox}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{tikz}
\usepgflibrary{arrows}
\usepackage[english]{babel}
\usepackage[autostyle, english = american]{csquotes}
\MakeOuterQuote{"}

\KOMAoptions{abstract=true}
\renewcommand{\labelitemi}{\scriptsize\color{blue!20!black}$\blacksquare$}

\definecolor{blue}{RGB}{30, 102, 245}
\definecolor{orange}{RGB}{221, 120, 120}
\definecolor{red}{RGB}{210, 15, 57}
\definecolor{green}{RGB}{64, 160, 43}
\definecolor{grey}{RGB}{140, 143, 161}
\definecolor{purple}{RGB}{136, 57, 239}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\definecolor{txtblue}{RGB}{52, 152, 219}
\definecolor{txtgray}{RGB}{171, 178, 185}

\definecolor{mygrey}{RGB}{242, 244, 244}
\definecolor{myblue}{RGB}{40, 116, 166}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{RGB}{44, 62, 80}
\definecolor{myorange}{RGB}{170, 121, 22}

%% code boxes
\newcommand{\col}[2]{\textcolor{#1}{#2}}
\newcommand{\code}[1]{\Colorbox{grey}{\texttt{#1}}}
\newcommand{\cdb}[1]{\texttt{#1}}
\newcommand{\cmt}[1]{\textcolor{codegreen}{\# \texttt{#1}}}

%% code highlighting
\newcommand{\fun}[1]{\Colorbox{grey}{\col{blue}{#1}}}
\newcommand{\var}[1]{\col{or}{#1}}
\newcommand{\key}[1]{\Colorbox{grey}{\col{red}{#1}}}
\newcommand{\obj}[1]{\Colorbox{grey}{\col{green}{#1}}}
\newcommand{\str}[1]{\col{purple}{#1}}

%% underline
\makeatletter
\DeclareRobustCommand*\myul{%
    \def\SOUL@everyspace{\underline{\space}\kern\z@}
    \def\SOUL@everytoken{%
     \setbox0=\hbox{\the\SOUL@token}%
     \ifdim\dp0>\z@
        \the\SOUL@token
     \else
        \underline{\the\SOUL@token}%
     \fi}
\SOUL@}
\makeatother

%% setting lengths
\setlength{\itemsep}{0em}
\setlength{\fboxsep}{2pt}
\setlength{\parindent}{2em}
\setlength{\parskip}{0em}

\counterwithin{equation}{section}
\hypersetup{
    colorlinks=true,
    linkcolor=cyan!80!blue,
    filecolor=magenta,      
    % urlcolor=black!20!cyan!80!blue,
    urlcolor=blue,
    citecolor=violet,
    linktoc=all
}
\newcommand{\mylink}[2]{\href{{#1}}{\texttt{#2}}}
% \addbibresource{references.bib}

\newtcolorbox{mybox}[3][]{
    colframe = #2!25,
    colback  = #2!10,
    coltitle = #2!20!black,  
    title    = {\textbf{#3}},
    #1,
    arc = 3pt
} 

\subject{\vspace*{-4em}}
\title{\LARGE {Advanced Machine Learning}}
\subtitle{\Large {Assignment 1}}
\author{\vspace*{-3.8em}}
\date{}

\begin{document}

    %% title page
    \maketitle
    \vspace*{-2em}
    \begin{table}[H]
        \centering
        \begin{tabular}{ccc}
            \begin{tabular}{@{}c@{}}
                \textbf{Roudranil Das} \\
                MDS202227 \\
                \texttt{\href{mailto:roudranil@cmi.ac.in}{roudranil@cmi.ac.in}}
            \end{tabular} & 
            \begin{tabular}{@{}c@{}}
                \textbf{Saikat Bera} \\
                MDS202228 \\
                \texttt{\href{mailto:saikatb@cmi.ac.in}{saikatb@cmi.ac.in}}
            \end{tabular} &
            \begin{tabular}{@{}c@{}}
                \textbf{Shreyan Chakraborty} \\
                MDS202237 \\
                \texttt{\href{mailto:shreyanc@cmi.ac.in}{shreyanc@cmi.ac.in}}
            \end{tabular} \\
            & \qquad &
        \end{tabular}\\[-3ex]
    \end{table}
    \hrule
    \vspace*{0.8mm}
    \hrule
    \tableofcontents

    \section{About the datasets}

    In the given datasets we have images consisting of hand gestures corresponding to the english sign language and images consisting of dressing items. The images are \(28\times 28\) in resolution.

    \section{Processing the dataset}

    The images were loaded from the corresponding csv file, where individual images are stored as rows, the columns being the pixel values. The images were then loaded as tensors and then normalised -
    \begin{itemize}
        \item for fashion mnist, we normalised the pixel values to 0 mean and 1 standard deviation
        \item for sign language mnist we divided all pixel values by 256 to bring the values between 0 and 1
    \end{itemize}

    \section{Fully connected NN vs CNN}

    We designed, trained and compared the performance of 2 basic DNN and CNN architectures on both the datasets. However even though the architectures are simplistic, when comparing the performances of the 2 models on the datasets, it is evident that CNN's are the better fit for image data.

    CNN's are far more able to capture the local patterns and the hierarchies in the images - which implies that they are able to extract better features from the images.

    \section{Comparing results}

    To compare results we trained both models for 10-30 epochs, and compared the results.

    \begin{table}[H]
        \centering
        \begin{tabular}{c|c|c|c|c|c}
            ~ & \multicolumn{2}{c|}{Loss} & \multicolumn{2}{c|}{Accuracy} &  \\ 
            Model & Best & Last & Best & Last & Time/Epoch. \\ \hline\hline
            DNN & 0.3045 & 0.3154 & 89 & 89 & 18s \\
            CNN & 0.2136 & 0.26 & 93 & 92 & 24s \\
        \end{tabular}
        \caption{Fashion mnist}
    \end{table}

    \begin{table}[H]
        \centering
        \begin{tabular}{c|c|c|c|c|c}
            ~ & \multicolumn{2}{c|}{Loss} & \multicolumn{2}{c|}{Accuracy} &  \\ 
            Model & Best & Last & Best & Last & Time/Epoch. \\ \hline\hline
            DNN & 1.193 & 1.358 & 68.12 & 66.13 & 60s \\
            CNN & 0.786 & 1.363 & 80.73 & 79.48 & 61s \\
            CNN with dropout & 0.377 & 0.419 & 90.31 & 89.3 & 62s \\
        \end{tabular}
        \caption{Sign language mnist}
    \end{table}

\section{Dependence of CNN on visual information}

CNN's rely on the visual information and patterns present in images to a varying degree. We can judge this by seeing if the models trained on a permutation of the pixels of the original images performs the same as the model trained on the original images.

For fashion mnist, which contains fairly simple images, there was not much difference in this scrambling - accuracy reduced from 93\% to 87\%.

However for sign language mnist, there was a noticeable drop - from 90\% to 52\%.

\section{FGSM attack on the networks}

Here is the performance of the FGSM attack on the 2 datasets

\centering
\begin{figure}[H]
    \includegraphics[width=15cm]{../data/fashionmnist-fgsm.png}
    \caption{Fashion mnist CNN}
\end{figure}

\begin{figure}[H]
    \includegraphics[width=15cm]{../data/signlanuage-fgsm.png}
    \caption{Sign language mnist CNN}
\end{figure}


\end{document}